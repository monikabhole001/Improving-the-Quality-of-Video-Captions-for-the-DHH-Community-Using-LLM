{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b504c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\p8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\ProgramData\\anaconda3\\envs\\p8\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "C:\\ProgramData\\anaconda3\\envs\\p8\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're going to make a ganache. we're going to\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load T5 Base model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example of using the model and tokenizer\n",
    "# input_text = \"translate English to French: Hello, how are you?\"\n",
    "prompt  = \"Correct the following caption as per english standard. Do not give additional information.\"\n",
    "caption = \"\"\"Caption:\n",
    "Today we're going to be mking easiest chocolte cake ever. You don't need any eggs so you don't even need an oven. It only takes five minutes to make in the microwave and it's so moist and so delicious. So let's begin. We're going to add our plain flour, our granulated sugar, our cocoa powder and our baking powder to a large bowl and then we're just going to whisk until combined. Now we're fully mixed we're going to add melted butter and warm water. So in goes the butter. You could also swap the butter for a neutral tasting oil to make the sponge completely dairy free and now the warm water. Now we're going to grab our whisk and we're going to whisk this until it's smooth and nice and ready. Okay so that is looking good. Now it's ready to go into our mould. Now you can use any microwave safe container. I'm using this seven inch silicone one that I got from Amazon. I've just sprayed it and lies the base of parchment paper. So now we're going to cook it. The time that it takes to cook will depend on the size of your container and also the material of it as well. I'm going to cook mine in the microwave of course on a medium heat about 500 watts for five to six minutes until a toothpick comes out with a few moist crumbs on. Our cake is now baked so we're going to test it with our toothpick or skewer to make sure that it comes out clean or with a few moist crumbs on. Perfect. Okay now I'm going to flip this. Usually I will take the plate to that and then flip but this is too heavy so I'm just going to try my best just to flip it straight on. Wish me luck. Ta-da! For the topping we're going to make a super easy rich chocolate ganache. All you need is dark chocolate and hot double cream or heavy cream. So we're going to pour our hot cream over our chocolate and then we're going to let it stand for a few minutes. Now the chocolate has started to melt so we're going to stir it gently until it comes together and it's nice and smooth. Our chocolate ganache is ready. If you didn't want to make a chocolate ganache you could always make a chocolate buttercream frosting or like a simple chocolate glaze or you can just spread the teller over your cake. Whatever you fancy I just like a chocolate ganache because it makes it extra chocolatey and it also looks really fancy. Now I'm just going to pour the ganache over the cake and we're going to put it that looks so good. I'm just going to smooth out and let it drip down the edges slightly. Now for the best part the taste test. Oh my gosh it's so good. It's so moist and chocolatey and fudgey. It tastes like a really rich chocolate fudge cake. I've got chocolate on my mouth have a nice. Thank you guys so much for watching this video. If you enjoyed it and you like super easy fun recipes like this one don't forget to like and subscribe and I'll see you in the next one. Bye!\"\"\"\n",
    "input_text = prompt + \"\\n\" + caption\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6923b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P8",
   "language": "python",
   "name": "p8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
